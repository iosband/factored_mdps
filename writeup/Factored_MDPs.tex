\documentclass{article}

%---------------------------------------------------------------------------------------------------------------------------------------------
% Including Packages
\usepackage{nips14submit_e,times}
\usepackage{color}
\usepackage{algorithm, algorithmic}
\usepackage{ mathrsfs }
\usepackage{ dsfont }
\usepackage{lmodern}
\usepackage{array}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[mathscr]{eucal}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{psfrag}
\usepackage{color}
\usepackage{here}
%\usepackage[margin=0.85in]{geometry}
\usepackage{wasysym}
%\usepackage[T1]{fontenc}
%\usepackage[charter]{mathdesign}
%\usepackage{eulervm}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsthm}
%\usepackage{ntheorem}
\usepackage{lipsum}
%\setlength{\parskip}{2pt}%
%\setlength{\parindent}{12pt}%


%\theoremstyle{break}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{mydef}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{claim}{Claim}
%---------------------------------------------------------------------------------------------------------------------------------------------
% Setting up macro shortcuts
\newcommand{\Exp}{\mathds{E}}
\newcommand{\Expk}{\mathds{E}_{k}}
\newcommand{\Prob}{\mathds{P}}
\newcommand{\Real}{\mathds{R}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\Ind}{\mathds{1}}

\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Dep}{ \Delta^H,\Delta^F,\mathcal{F},\epsilon }

\newcommand{\conf}{\mathcal{F}^d_t}

\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\opt}{M^*}
\newcommand{\sampled}{{M_k}}
\newcommand{\Pstar}{P^{*}(\cdot \mid s_t, a_t)}
\newcommand{\Pk}{P_{k}(\cdot \mid s_t, a_t)}
\newcommand{\Pdiff}{(P_{k}-P^{*})(\cdot \mid s_t, a_t)}
\newcommand{\Rdiff}{(r_k-r^{*})(s_t, a_t)}
\newcommand{\optPol}{\mu^{*}}
\newcommand{\sampledPol}{\mu_{k}}
\newcommand{\bellmanSampled}{\mathcal{T}_{\mu_{k}(\cdot,i)}^{k}}
\newcommand{\bellmanTrue}{\mathcal{T}_{\mu_{k}(\cdot,i)}^{*}}
\newcommand{\bellmanSampledA}{\mathcal{T}_{\mu_{k}(\cdot,1)}^{k}}
\newcommand{\bellmanTrueA}{\mathcal{T}_{\mu_{k}(\cdot,1)}^{*}}
\newcommand{\vSampled}{V_{\mu_k, 1}^{k}}
\newcommand{\vSampledi}{V_{\mu_k, i}^{k}}
\newcommand{\vTrue}{V_{\tau, \mu_k}^{*}}

%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\title{Near-optimal Reinforcement Learning \\ in Factored MDPs}

\author{
Ian Osband \\
Stanford University\\
\texttt{iosband@stanford.edu} \\
\And
Benjamin Van Roy \\
Stanford University \\
\texttt{bvr@stanford.edu}
}

\nipsfinalcopy

\begin{document}
\maketitle


%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\begin{abstract}
Any reinforcement learning algorithm that applies to all Markov decision processes (MDPs) will suffer $\Omega(\sqrt{SAT})$ regret on some MDP, where $T$ is the elapsed time and $S$ and $A$ are the cardinalities of the state and action spaces.
This implies $T = \Omega(SA)$ time to guarantee a near-optimal policy.
In many settings of practical interest, due to the curse of dimensionality, $S$ and $A$ can be so enormous that this learning time is unacceptable.
We establish that, if the system is known to be a \emph{factored} MDP, it is possible to achieve regret that scales polynomially in the number of \emph{parameters} encoding the factored MDP, which may be exponentially smaller than $S$ or $A$.
We provide two algorithms that satisfy near-optimal regret bounds in this context:
posterior sampling reinforcement learning (PSRL) and an upper confidence bound algorithm (UCRL-Factored).
\end{abstract}


%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Introduction}

We consider a reinforcement learning agent that takes sequential actions within an uncertain environment with an aim to maximize cumulative reward \cite{burnetas1997optimal}.
We model the environment as a Markov decision process (MDP) whose dynamics are not fully known to the agent.
The agent can learn to improve future performance by exploring poorly-understood states and actions, but might improve its short-term rewards through a policy which exploits its existing knowledge.
Efficient reinforcement learning balances exploration with exploitation to earn high cumulative reward.

The vast majority of efficient reinforcement learning has focused upon the {\it tabula rasa} setting, where little prior knowledge is available about the environment beyond its state and action spaces.
In this setting several algorithms have been designed to attain sample complexity polynomial in the number of states $S$ and actions $A$ \cite{kearns2002near, brafman2003r}.
Stronger bounds on regret, the difference between an agent's cumulative reward and that of the optimal controller, have also been developed.
The strongest results of this kind establish $\tilde{O}(S\sqrt{AT})$ regret for particular algorithms \cite{jaksch2010near,bartlett2009regal,osband2013more} which is close to the lower bound $\Omega(\sqrt{SAT})$ \cite{jaksch2010near}.
However, in many setting of interest, due to the curse of dimensionality, $S$ and $A$ can be so enormous that even this level of regret is unacceptable.

In many practical problems the agent \emph{will} have some prior understanding of the environment beyond {\it tabula rasa}.
For example, in a large production line with $m$ machines in sequence each with $K$ possible states, we may know that over a single time-step each machine can only be influenced by its direct neighbors.
Such simple observations can reduce the dimensionality of the learning problem exponentially, but cannot easily be exploited by a {\it tabula rasa} algorithm.
Factored MDPs (FMDPs) \cite{boutilier2000stochastic}, whose transitions can be represented by a dynamic Bayesian network (DBN) \cite{ghahramani1998learning}, are one effective way to represent these structured MDPs compactly.

Several algorithms have been developed that exploit the known DBN structure to achieve sample complexity polynomial in the \emph{parameters} of the FMDP, which may be exponentially smaller than $S$ or $A$ \cite{strehl2007model,kearns1999efficient,szita2009optimistic}.
However, these polynomial bounds include several high order terms.
% Some algorithms seek to learn this structure from experience \cite{strehl2007efficient}, but we will assume that this is known a priori.
We present two algorithms, UCRL-Factored and PSRL, with the first near-optimal regret bounds for factored MDPs.
UCRL-Factored is an optimistic algorithm that modifies the confidence sets of UCRL2 \cite{jaksch2010near} to take advantage of the network structure.
PSRL is motivated by the old heuristic of Thompson sampling \cite{thompson1933} and has been previously shown to be efficient in non-factored MDPs \cite{strens2000bayesian,osband2013more}.
These algorithms are descibed fully in Section \ref{sec: algos}.

Both algorithms make use of approximate FMDP planner in internal steps.
However, even where an FMDP can be represented concisely, solving for the optimal policy may take exponentially long in the most general case \cite{guestrin2003efficient}.
Our focus in this paper is upon the statistical aspect of the learning problem and like earlier discussions we do not specify which computational methods are used \cite{kearns1999efficient}.
Our results serve as a reduction of the reinforcement learning problem to finding an approximate solution for a given FMDP.
In many cases of interest, effective approximate planning methods for FMDPs do exist.
Investigating and extending these methods are an ongoing subject of research \cite{koller2000policy, guestrin2001max,delgado2011efficient,sanner2012approximate}.

% We believe that dimensionality reduction in large MDPs is essential for practical reinforcement learning.
% Factored MDPs are an approach with successful applications in many fields \cite{boutilier2000stochastic} but they are not the only one.
% There are regret bounds for continuous state spaces where the underlying reward and transition functions are known to belong to some function class, such as H\"{o}der continuous \cite{ortner2012online} or linear quadratic control \cite{abbasi2011regret}.
% These results are interesting, but each have some undesirable properties, the former has regret bounds which approach $O(T)$ for high dimensions and the latter retains an exponential dependence on the dimension.
% Perhaps the most popular approach in the literature is to assume the \emph{value} function can be well-approximated by a low-dimensional (usually linear) representation of basis functions.
% Value-based approaches typically struggle to plan efficient exploration and so cannot obtain efficient learning guarantees, although there has been interesting progress in this field as well \cite{van2014generalization}.



%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Problem formulation}

We consider the problem of learning to optimize a random finite horizon MDP $M = (\Sc, \Ac, R^M, P^M, \tau, \rho)$ in repeated finite episodes of interaction.
$\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $R^M(s,a)$ is the reward distibution over $\Real$ in state $s$ with action $a$, $P^M(\cdot|s,a)$ is the transition probability over $\Sc$ from state $s$ with action $a$, $\tau$ is the time horizon, and $\rho$ the initial state distribution.
We define the MDP and all other random variables we will consider with respect to a probability space $(\Omega, \mathcal{F}, \mathbb{P})$.

A deterministic policy $\mu$ is a function mapping each state $s \in \Sc$ and $i = 1,\ldots,\tau$ to an action $a \in \Ac$.
For each MDP $M = (\mathcal{S}, \mathcal{A}, R^M, P^M, \tau, \rho)$ and policy $\mu$, we define a value function
$$V^{M}_{\mu, i}(s) := \Exp_{M,\mu}\left[ \sum_{j=i}^{\tau} \overline{R}^M(s_j,a_j) \Big| s_i = s \right],$$
where $\overline{R}^M(s,a)$ denotes the expected reward realized when action $a$ is selected while in state $s$, and the subscripts of the expectation operator indicate that $a_j = \mu(s_j, j)$, and $s_{j+1} \sim P^M(\cdot| s_j, a_j)$ for $j = i, \ldots, \tau$.
A policy $\mu$ is optimal for the MDP $M$ if $V^{M}_{\mu, i}(s) = \max_{\mu'} V^{M}_{\mu', i}(s)$ for all $s \in \Sc$ and $i=1,\ldots,\tau$. We will associate with each MDP $M$ a policy $\mu^M$ that is optimal for $M$.

The reinforcement learning agent interacts with the MDP over episodes that begin at times $t_k = (k-1) \tau + 1$, $k=1,2,\ldots$.
At each time $t$, the agent selects an action $a_t$, observes a scalar reward $r_t$, and then transitions to $s_{t+1}$.
Let $H_t = (s_1,a_1,r_1,\ldots,s_{t-1},a_{t-1},r_{t-1})$ denote the history of observations made \emph{prior} to time $t$.
A reinforcement learning algorithm is a deterministic sequence $\{\pi_k | k = 1, 2, \ldots\}$ of functions, each mapping $H_{t_k}$ to a probability distribution $\pi_{k}(H_{t_k})$ over policies which the agent will employ during the $k$th episode.
We define the regret incurred by a reinforcement learning algorithm $\pi$ up to time $T$ to be:
$${\rm Regret}(T, \pi, M^*) := \sum_{k=1}^{\lceil T/\tau \rceil} \Delta_k,$$
where $\Delta_k$ denotes regret over the $k$th episode, defined with respect to the MDP $M^*$ by
$$\Delta_k := \sum_{\Sc} \rho(s) (V^{M^*}_{\mu^*, 1}(s) - V^{M^{*}}_{\mu_k, 1}(s))$$
with $\mu^* = \mu^{M^*}$ and $\mu_{k}\sim \pi_{k}(H_{t_k})$. Note that regret is not deterministic since it can depend on the random MDP $M^*$, the algorithm's internal random sampling and, through the history $H_{t_k}$, on previous random transitions and random rewards. We will assess and compare algorithm performance in terms of regret and its expectation.




%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Factored MDPs}

Intuitively a factored MDP is an MDP whose rewards and transitions exhibit some conditional independence structure.
To formalize this definition we must introduce some more notation common to the literature \cite{szita2009optimistic}.

\begin{mydef}[Scope operation for factored sets $\Xc = \Xc_1 \times .. \times \Xc_n$]
\hspace{0.000000001mm} \newline
For any subset of indices $Z \subseteq \{1,2,..,n\}$ let us define the scope set $\Xc[Z] := \bigotimes\limits_{i \in Z} \Xc_i$.
Further, for any $x \in \Xc$ define the scope variable $x[Z] \in \Xc[Z] $ to be the value of the variables $x_i \in \Xc_i$ with indices $i \in Z$.
For singleton sets $Z$ we will write $x[i]$ for $x[\{ i \}]$ in the natural way.
\end{mydef}


Let $\Pc_{\Xc,\Yc}$ be the set of functions mapping elements of a finite set $\Xc$ to probability mass functions over a finite set $\Yc$.
$\Pc^{C,\sigma}_{\Xc,\Real}$ will denote the set of functions mapping elements of a finite set $\Xc$ to $\sigma$-sub-Gaussian probability measures over $(\Real, \mathcal{B}(\Real))$ with mean bounded in $[0,C]$.
For reinforcement learning we will write $\Xc$ for $\Sc \times \Ac$ and consider factored reward and factored transition functions which are drawn from within these families.

\begin{mydef}[ Factored reward functions $R \in \Rc \subseteq \Pc^{C,\sigma}_{\Xc,\Real} $]
\hspace{0.000000001mm} \newline
The reward function class $\Rc$ is factored over $\Sc \times \Ac = \Xc = \Xc_1 \times .. \times \Xc_n $ with scopes $Z_1, .. Z_l$ if and only if,
for all $R \in \Rc, x \in \Xc$ there exist functions $\{R_i \in \Pc^{C,\sigma}_{\Xc [ Z_i] ,\Real} \}_{i=1}^l $ such that,
$$ \Exp [ r ] = \sum_{i=1}^l \Exp\big[ r_i \big] $$
for $r \sim R(x)$ is equal to $\sum_{i=1}^l r_i$ with each $r_i \sim R_i(x[Z_i])$ and individually observed.
\end{mydef}


\begin{mydef}[ Factored transition functions $P \in \Pc \subseteq \Pc_{\Xc,\Sc}$ ]
\hspace{0.000000001mm} \newline
The transition function class $\Pc$ is factored over $\Sc \times \Ac = \Xc = \Xc_1 \times .. \times \Xc_n$ and $\Sc = \Sc_1 \times .. \times \Sc_m$ with scopes  $Z_1, .. Z_m$ if and only if,
for all $P \in \Pc, x \in \Xc, s \in \Sc$ there exist some $\{P_i \in \Pc_{\Xc[Z_i],\Sc_i}\}_{i=1}^m$ such that,
$$ P(s | x) = \prod_{i=1}^m P_i \left( s[i] \ \bigg\vert \ x[Z_i] \right) $$
\end{mydef}

A factored MDP (FMDP) is then defined to be an MDP with both factored rewards and factored transitions.
Writing $\Xc = \Sc \times \Ac$ a FMDP is fully characterized by the tuple
$$ M = \big( \{ \Sc_i \}_{i=1}^m ; \  \{ \Xc_i \}_{i=1}^n ; \   \{ Z^R_i \}_{i=1}^l;\  \{ R_i \}_{i=1}^l;\  \{ Z^P_i \}_{i=1}^m;\  \{ P_i \}_{i=1}^m;\  \tau;\  \rho  \big), $$
where $Z^R_i$ and $Z^P_i$ are the scopes for the reward and transition functions respectively in $\{1,..,n \}$ for $\Xc_i$.
We assume that the size of all scopes $| Z_i | \le \zeta \ll n$ and factors $|\Xc_i| \le K$ so that the domains of $R_i$ and $P_i$ are of size at most $K^\zeta$.


%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Results}

% We present two algorithms, PSRL and UCRL-Factored with efficient regret bounds over factored MDPs.
% PSRL is guided by posterior sampling while UCRL-Factored uses optimism in the face of uncertainty.
% Full details of these algorithms are available in Section \ref{sec: algos}.

Our first result shows that we can bound the expected regret of PSRL.

\begin{theorem}[Expected regret for PSRL in factored MDPs]
\label{thm: reg PSRL}  \hspace{0.000000001mm} \newline
Let $M^*$ be factored with graph structure $\Gc  = \big( \{ \Sc_i \}_{i=1}^m ; \  \{ \Xc_i \}_{i=1}^n ; \   \{ Z^R_i \}_{i=1}^l;\  \{ Z^P_i \}_{i=1}^m;\  \tau \big)$.
If $\phi$ is the distribution of $M^*$ and $\Psi$ is the span of the optimal value function then we can bound the regret of PSRL:
% {\tiny
\begin{eqnarray}
	\Exp \left[\mathrm{Regret}(T, \pi^{\rm PS}_{\tau}, M^*) \right]
        \hspace{-3mm} &\le& \hspace{-2mm}
		\sum_{i=1}^l \left\{ 5\tau C |\Xc[Z^R_i]| + 12\sigma\sqrt{| \Xc[Z^R_i]| T \log\left(4 l | \Xc[Z^R_i] | k T\right)} \right\} \nonumber +2 \sqrt{T} \\
	&& \hspace{-36mm} + 4 + \Exp[\Psi] \left( 1 + \frac{4}{T-4} \right) \sum_{j=1}^m \left\{ 5\tau |\Xc[Z^P_j]| + 12\sqrt{| \Xc[Z^P_j]| |\Sc_j |   T \log\left(4 m | \Xc[Z^P_j] | k T\right)} \right\}
\end{eqnarray}
% }
\end{theorem}

We have a similar result for UCRL-Factored that holds with high probability.
\begin{theorem}[High probability regret for UCRL-Factored in factored MDPs]
\label{thm: reg UCRL-Factored}  \hspace{0.000000001mm} \newline
Let $M^*$ be factored with graph structure $\Gc  = \big( \{ \Sc_i \}_{i=1}^m ; \  \{ \Xc_i \}_{i=1}^n ; \   \{ Z^R_i \}_{i=1}^l;\  \{ Z^P_i \}_{i=1}^m;\  \tau \big)$.
If $D$ is the diameter of $M^*$, then for any $M^*$ can bound the regret of UCRL-Factored:
% {\tiny
\begin{eqnarray}
	\mathrm{Regret}(T, \pi^{\rm UC}_{\tau}, M^*)
    \hspace{-3mm} &\le& \hspace{-2mm}
    \sum_{i=1}^l \left\{ 5\tau C |\Xc[Z^R_i]| + 12\sigma\sqrt{ | \Xc[Z^R_i]| T \log\left(12 l | \Xc[Z^R_i] | k T / \delta\right)}  \right\} \nonumber + 2\sqrt{T}\\
	&& \hspace{-37mm} + CD\sqrt{2T \log(6/\delta)}  + \  CD \sum_{j=1}^m \left\{ 5\tau |\Xc[Z^P_j]| + 12\sqrt{| \Xc[Z^P_j]| |\Sc_j |   T \log\left(12 m | \Xc[Z^P_j] | k T / \delta \right)} \right\}
\end{eqnarray}
% }
\normalsize
with probability at least $1-\delta$
\end{theorem}

Both algorithms give bounds $\tilde{O}\left(\Xi \sum_{j=1}^m \sqrt{|\Xc[Z_j^P]| |S_j| T}\right)$ where $\Xi$ is a measure of MDP connectedness: expected span $\Exp[\Psi]$ for PSRL and scaled diameter $CD$ for UCRL-Factored.
The span of an MDP is the maximum difference in value of any two states under the optimal policy $\Psi(M^*) := \max_{s,s' \in \Sc} \{V^{M^*}_{\mu^*,1}(s) - V^{M^*}_{\mu^*,1}(s')\}$.
The diameter of an MDP is the maximum number of expected timesteps to get between any two states $D(M^*) = \max_{s \neq s'}\min_\mu T^\mu_{s \rightarrow s'}$.
PSRL's bounds are tighter since $\Psi(M) \le C D(M)$ and may be exponentially smaller.

However, UCRL-Factored has stronger probabilistic guarantees than PSRL since its bounds hold with high probability for any MDP $M^*$ not just in expectation.
There is an optimistic algorithm REGAL \cite{bartlett2009regal} which formally replaces the UCRL2 $D$ with $\Psi$ and retains the high probability guarantees.
An analogous extension to REGAL-Factored is possible, however, no practical implementation of that algorithm exists even with an FMDP planner.

The algebra in Theorems \ref{thm: reg PSRL} and \ref{thm: reg UCRL-Factored} can be overwhelming.
For clarity, we present a symmetric problem instance for which we can produce a cleaner single-term upper bound.
Let $\Qc$ be shorthand for the simple graph structure with $l+1=m$, $C=\sigma=1$, $| \Sc_i | = |\Xc_i | = K$ and $|Z^R_i| = |Z^P_j| = \zeta$ for $i=1,..,l$ and $j=1,..,m$, we will write $J = K^\zeta$.
% In this case $\Psi, D \le \tau$ trivially.

\begin{corollary}[Clean bounds for PSRL in a symmetric problem]
\label{cor: reg PSRL}  \hspace{0.000000001mm} \newline
If $\phi$ is the distribution of $M^*$ with structure $\Qc$ then we can bound the regret of PSRL:
\begin{equation}
	\Exp \left[\mathrm{Regret}(T, \pi^{\rm PS}_{\tau}, M^*) \right]
	\le 15m \tau \sqrt{J K T \log(2mJ T)}
\end{equation}
\end{corollary}

\begin{corollary}[Clean bounds for UCRL-Factored in a symmetric problem]
\label{cor: reg UCRL-Factored}  \hspace{0.000000001mm} \newline
For any MDP $M^*$ with structure $\Qc$ we can bound the regret of UCRL-Factored:
\begin{equation}
	\mathrm{Regret}(T, \pi^{\rm UC}_{\tau}, M^*) \le  15m \tau \sqrt{J K T \log(12mJT / \delta)}
\end{equation}
with probability at least $1-\delta$.
\end{corollary}


Both algorithms satisfy bounds of $\tilde{O}(\tau m \sqrt{JKT})$ which is exponentially tighter than can be obtained by any $\Qc$-naive algorithm.
For a factored MDP with $m$ independent components with $S$ states and $A$ actions the bound $\tilde{O}(m S \sqrt{AT})$ is close to the lower bound $\Omega(m\sqrt{SAT})$ and so the bound is near optimal.
The corollaries follow directly from Theorems \ref{thm: reg PSRL} and \ref{thm: reg UCRL-Factored} as shown in Appendix \ref{sec: Clean symmetric bounds}.

% \subsection{Interpretting regret bounds}

% The bounds for PSRL and UCRL-Factored are qualitatively similar and share much of the same analysis.
% For each algorithm, the regret is $\tilde{O}\left(\Xi \sum_{j=1}^m \sqrt{|\Xc[Z_j^P]| |S_j| T}\right)$ where $\Xi$ is a measure of MDP connectedness, expected span $\Exp[\Psi]$ for PSRL and scaled diameter $CD$ for UCRL-Factored.

% The span of an MDP is defined $\Psi(M^*) := \max_{s,s' \in \Sc} \{V^{M^*}_{\mu^*,1}(s) - V^{M^*}_{\mu^*,1}(s')\}$ which is the maximum difference in expected value of any two states under the optimal policy.
% The diameter of an MDP $D(M^*) = \max_{s \neq s'}\min_\mu T^\mu_{s \rightarrow s'}$, where $T^\mu_{s \rightarrow s'}$ is the expected number of steps to get from $s$ to $s'$ under policy $\mu$.
% It is always the case that $\Psi(M) \le C D(M)$, otherwise one might improve the optimal policy from $s'$ to follow simply by taking the fastest policy to the $s$ with highest value.
% In some cases the span may be exponentially smaller than the diameter.
% In this sense PSRL satisfies a tighter bound.

% However, UCRL-Factored has stronger probabilistic guarantees than PSRL since its bounds hold with high probability for any MDP $M^*$ not just in expectation.
% There is an optimistic algorithm REGAL \cite{bartlett2009regal} which formally replaces the UCRL2 $D$ with $\Psi$ and retains the high probability guarantees.
% However, no practical implementation of that algorithm exists, even when given access to an MDP planner.
% An analogous extension to the analysis of REGAL-Factored is possible.

% We should also note that UCRL2 was designed to obtain regret bounds even in MDPs without episodic reset.
% This is accomplished by imposing artificial episodes which end whenever the number of visits to a state-action pair is doubled \cite{jaksch2010near}.
% Using a similar modification, it is possible to extend UCRL-Factored to this setting without trouble and retain similar regret bounds.
% However, this doubling trick in PSRL does not retain provable regret bounds, since the episode length is no longer independent of the sampled MDP.
% Nevertheless, there has been good empirical performance using this method for non-factored MDPs without episodic reset in simulation \cite{osband2013more}.


%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Confidence sets}
Our analysis will rely upon the construction of confidence sets based around the empirical estimates for the underlying reward and transition functions.
The confidence sets are constructed to contain the true MDP with high probability.
This technique is common to the literature, but we will exploit the additional graph structure $\Gc$ to sharpen the bounds.

Consider a family of functions $\Fc \subseteq \Mc_{\Xc,(\Yc,\Sigma_\Yc)}$ which takes $x \in \Xc$ to a probability distribution over $(\Yc, \Sigma_\Yc)$.
We will write $\Mc_{\Xc,\Yc}$ unless we wish to stress a particular $\sigma$-algebra.
\begin{mydef}[Set widths] \hspace{0.000000001mm} \newline
Let $\Xc$ be a finite set, and let $(\Yc,\Sigma_{\Yc})$ be a measurable space.  The {\it width} of a set $\Fc \in \Mc_{\Xc,\Yc}$ at $x \in \Xc$ with respect to
a norm $\|\cdot\|$ is
$$w_{\mathcal{F}}(x) := \sup_{\overline{f}, \underline{f} \in \mathcal{F}} \|(\overline{f} -  \underline{f})(x)\|$$
\end{mydef}

Our confidence set sequence $\{\Fc_t \subseteq \Fc : t \in \Nat\}$ is initialized with a set $\Fc$.
We adapt our confidence set to the observations $y_t \in \Yc$ which are drawn from the true function $f^* \in \Fc$ at measurement points $x_t \in \Xc$ so that $y_t \sim f^*(x_t)$.
Each confidence set is then centered around an empirical estimate $\hat{f}_t \in \Mc_{\Xc,\Yc}$ at time $t$, defined by
$$\hat{f}_t(x) = \frac{1}{n_t(x)} \sum_{\tau < t: x_\tau = x} \delta_{y_\tau},$$
where $n_t(x)$ is the number of time $x$ appears in $(x_1, .. ,x_{t-1})$ and $\delta_{y_t}$ is the probability mass function over $\Yc$ that assigns all probability to the outcome $y_t$.

Our sequence of confidence sets depends on our choice of norm $\| \cdot \|$ and a non-decreasing sequence $\{d_t : t \in \Nat\}$.
For each $t$, the confidence set is defined by:
$$\Fc_t = \Fc_t ( \| \cdot \|, x^{t-1}_1 , d_t):= \left\{f \in \Fc \ \bigg\vert \ \|(f - \hat{f}_t)(x_i)\| \leq \sqrt{\frac{d_t}{n_t(x_i)}} \ \forall i=1,..,t-1 \right\}.$$
Where $x^{t-1}_1$ is shorthand for $(x_1, .. ,x_{t-1})$ and we interpret $n_t(x_i) = 0$ as a null constraint.
The following result shows that we can bound the sum of confidence widths through time.


\begin{theorem}[Bounding the sum of widths]
\label{thm: widths} \hspace{0.000000001mm} \newline
% Let us write $\Fc_k$ for $\Fc_{t_k}$ and associate times within episodes of length $\tau$, $t = t_k+i$ for $i=1,..,\tau$ and $T = L \times \tau$.
For all finite sets $\Xc$, measurable spaces $(\Yc,\Sigma_{\Yc})$, function classes $\Fc \subseteq \Mc_{\Xc,\Yc}$ with uniformly bounded widths $w_\Fc(x) \le C_\Fc \ \forall x \in \Xc$ and non-decreasing sequences $\{d_t : t \in \Nat \}$:

\begin{equation}
	\sum_{k=1}^L \sum_{i=1}^\tau w_{\mathcal{F}_{k}}(x_{t_k+i})
	\le  4 \big(\tau C_\Fc | \Xc | + 1\big) + 4 \sqrt{2d_T | \Xc | T}
\end{equation}

\begin{proof}
The proof follows from elementary counting arguments on $n_t(x)$ and the pigeonhole principle.
A full derivation is given in Appendix \ref{sec: widths}.
\end{proof}
\end{theorem}




%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Algorithms}
\label{sec: algos}

With our notation established, we are now able to introduce our algorithms for efficient learning in Factored MDPs.
PSRL and UCRL-Factored proceed in episodes of fixed policies.
At the start of the $k$th episode they produce a candidate MDP $M_k$ and then proceed with the policy which is optimal for $M_k$.
In PSRL, $M_k$ is generated by a sample from the posterior for $M^*$, whereas UCRL-Factored chooses $M_k$ optimistically from the confidence set $\Mc_k$.

Both algorithms require prior knowledge of the graphical structure $\Gc$ and an approximate planner for FMDPs.
We will write $\Gamma(M,\epsilon)$ for a planner which returns $\epsilon$-optimal policy for $M$.
We will write $\tilde{\Gamma}(\Mc,\epsilon)$ for a planner which returns an $\epsilon$-optimal policy for most optimistic realization from a family of MDPs $\Mc$.
Given $\Gamma$ it is possible to obtain $\tilde{\Gamma}$ through extended value iteration, although this might become computationally intractable \cite{jaksch2010near}.

PSRL remains identical to earlier treatment \cite{strens2000bayesian, osband2013more} provided $\Gc$ is encoded in the prior $\phi$.
UCRL-Factored is a modification to UCRL2 that can exploit the graph and episodic structure of .
We write $\Rc^i_t(d_t^{R_i})$ and $\Pc^j_t(d_t^{P_j})$ as shorthand for these confidence sets
$\Rc^i_t( | \Exp[\cdot] |, x^{t-1}_1[Z^R_i],d_t^{R_i})$ and $\Pc^i_t( \| \cdot \|_1, x^{t-1}_1[Z^P_j],d_t^{P_j})$
generated from initial sets $\Rc^i_1 = \Pc^{C,\sigma}_{\Xc[Z^R_i],\Real}$ and $\Pc^j_1 = \Pc_{\Xc[Z^P_j],\Sc_j}$.

We should note that UCRL2 was designed to obtain regret bounds even in MDPs without episodic reset.
This is accomplished by imposing artificial episodes which end whenever the number of visits to a state-action pair is doubled \cite{jaksch2010near}.
It is simple to extend UCRL-Factored's guarantees to this setting using this same strategy.
This will not work for PSRL since our current analysis requires that the episode length is independent of the sampled MDP.
Nevertheless, there has been good empirical performance using this method for MDPs without episodic reset in simulation \cite{osband2013more}.


%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\begin{figure}[H]
\begin{minipage}[t]{2.25in}
\algsetup{indent=1em}
\begin{algorithm}[H]
\caption{\protect\\ PSRL (Posterior Sampling)}
\label{alg: PSRL}
{\small
\begin{algorithmic}[1]
	\STATE \textbf{Input: }Prior $\phi$ encoding $\Gc$, $t=1$
	\FOR{episodes $k=1,2,..$}
		\STATE{sample $M_k \sim \phi(\cdot | H_{t})$}
		\STATE{compute $\mu_k = \Gamma(M_k,\sqrt{\tau / k})$}

		\FOR{timesteps $j=1,..,\tau$}
			\STATE{sample and apply $a_t = \mu_k(s_t,j)$}
			\STATE{observe $r_t$ and $s^m_{t+1}$}
			\STATE{$t = t+1$}
		\ENDFOR

	\ENDFOR
\end{algorithmic}
}
\end{algorithm}

\end{minipage}
\hspace{0.25cm}
\begin{minipage}[t]{3.1in}
\centering

\algsetup{indent=1em}
\begin{algorithm}[H]
\caption{\protect\\ UCRL-Factored (Optimism)}
\label{alg: UCRL-Factored}
{\small
\begin{algorithmic}[1]
	\STATE \textbf{Input: }Graph structure $\Gc$, confidence $\delta$, $t=1$
	\FOR{episodes $k=1,2,..$}
		\STATE{$d_t^{R_i} = 4 \sigma^2 \log\left(4 l | \Xc[Z^R_i] | k / \delta\right)$ for $i=1,..,l$}
		\STATE{$d_t^{P_j} = 4 | \Sc_j | \log\left(4 m | \Xc[Z^P_j] | k  / \delta\right)$ for $j=1,..,m$}
		\STATE{ $\Mc_k = \{M \ | \Gc, \overline{R}_i \in \Rc^i_t(d_t^{R_i}), P_j \in \Pc^j_t(d_t^{P_j}) \ \forall i,j \}$}
		\STATE{compute $\mu_k = \tilde{\Gamma}(\Mc_k,\sqrt{\tau / k})$}

		\FOR{timesteps $u=1,..,\tau$}
			\STATE{sample and apply $a_t = \mu_k(s_t,u)$}
			\STATE{observe $r^1_t,..,r^l_t$ and $s^1_{t+1},..,s^m_{t+1}$}
			\STATE{$t = t+1$}
		\ENDFOR

	\ENDFOR
\end{algorithmic}
}
\end{algorithm}
\end{minipage}
\end{figure}

% The parameters $d_t^{R_i}$ and $d_t^{P_j}$ are chosen to satisfy concentration inequalities so that the true MDP $M^*$ lies within $\Mc_k$ for all $k$ with high probability.
% Although PSRL makes no mention of confidence sets, $\Mc_k$ will also be useful in the analysis of PSRL.





%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Analysis}
% We will now piece together the necessary analysis for our main results.
% First we recap the analysis of PSRL and UCRL2 which allow us to the regret to the bellman error.
% Next we show that, for factored MDPs, it is possible to bound this estimation error by the error in each factored component separately.
% From here we will use concentration inequalities upon the individual factors to show that, with high probability, the true MDP $M^*$ lies within $\Mc_k$ for all $k$.
% The final results will then be obtained through an application of Theorem \ref{thm: widths}.

% \subsection{From regret to Bellman error}
% A key difficulty in providing regret bounds for reinforcement learning is that it depends upon the rewards of the optimal policy $\mu^*$.
% For many reinforcement learning algorithms there is no clean way to relate the unknown optimal policy to the states and actions observed by the agent.
% Using the OFU principle, we can guarantee with high probability that the optimal rewards of the true MDP are upper bounded by the optimal rewards of the optimistic MDP \cite{kaelbling1996reinforcement}.
% In the case of posterior sampling, we make use of the posterior sampling lemma \cite{russo2013}

% \begin{lemma}[Posterior Sampling]
% \label{lem: fundamental}  \hspace{0.000000001mm} \newline
% If $\phi$ is the distribution of $M^*$ then, for any $\sigma(H_{t_k})$-measurable function $g$,
% \begin{equation}\label{eq: fundamental lemma}
% \Exp [g(M^*) | H_{t_k}] = \Exp[g(M_k) | H_{t_k}].
% \end{equation}
% \end{lemma}

% Note that taking the expectation of \eqref{eq: fundamental lemma} shows $\Exp [g(M^*)] = \Exp[g(M_k)]$ through the tower property.
% We introduce the Bellman operator $\mathcal{T}_{\mu}^{M}$, which for any MDP $M = (\Sc, \Ac, R^M, P^M, \tau, \rho)$, stationary policy $\mu:\Sc \rightarrow \Ac$ and value function $V:\Sc \rightarrow \mathds{R}$, is defined by
% $$\mathcal{T}_{\mu}^{M} V(s) := \overline{R}^{M} (s, \mu(s)) + \sum_{s' \in \Sc} P^{M}(s' | s, \mu(s)) V(s').$$
% This returns the expected value of state $s$ where we follow the policy $\mu$ under the laws of $M$, for one time step.
% The following lemma gives a concise form for the dynamic programming paradigm in terms of the Bellman operator.

% \begin{lemma}[Dynamic programming equation]
% \label{lem: DPl} \hspace{0.000000001mm} \newline
% For any MDP $M = (\Sc, \Ac, R^M, P^M, \tau, \rho)$ and policy $\mu:\Sc \times \{1,\ldots,\tau\} \rightarrow \Ac$, the value functions $V^M_\mu$ satisfy
% \begin{equation}\label{eq: DP}
% V_{\mu,i}^{M} = \mathcal{T}_{\mu(\cdot,i)}^M V_{\mu,i+1}^M
% \end{equation}
% for $i=1\dots \tau$, with $V_{\mu,\tau+1}^{M} := 0$.
% \end{lemma}

For our common analysis of PSRL and UCRL-Factored we will let $\tilde{M}_k$ refer generally to either the sampled MDP used in PSRL or the optimistic MDP chosen from $\Mc_k$ with associated policy $\tilde{\mu}_k$).
We introduce the Bellman operator $\mathcal{T}_{\mu}^{M}$, which for any MDP $M = (\Sc, \Ac, R^M, P^M, \tau, \rho)$, stationary policy $\mu:\Sc \rightarrow \Ac$ and value function $V:\Sc \rightarrow \mathds{R}$, is defined by
$$\mathcal{T}_{\mu}^{M} V(s) := \overline{R}^{M} (s, \mu(s)) + \sum_{s' \in \Sc} P^{M}(s' | s, \mu(s)) V(s').$$
This returns the expected value of state $s$ where we follow the policy $\mu$ under the laws of $M$, for one time step.
We will streamline our discussion of $P^{M}, R^{M}, V^{M}_{\mu,i}$ and $\Tc^M_\mu$ by simply writing $*$ in place of $M^*$ or $\mu^*$ and $k$ in place of $\tilde{M}_k$ or $\tilde{\mu}_k$ where appropriate; for example $V^*_{k,i} := V^{M^*}_{\tilde{\mu}_k,i}$. We will also write $x_{k,i} := \left(s_{t_k+i}, \mu_k(s_{t_k+i})\right)$.

We now break down the regret by adding and subtracting the \emph{imagined} near optimal reward of policy $\tilde{\mu}_K$, which is known to the agent.
For clarity of analysis we consider only the case of $\rho(s') = \Ind \{s' = s \}$ but this changes nothing for our consideration of finite $\Sc$.
\begin{equation}
	\Delta_k =  V^*_{*,1}(s) - V^*_{k,1}(s) = \bigg( V^k_{k,1}(s) - V^*_{k,1}(s) \bigg) + \bigg(V^*_{*,1}(s) - V^k_{k,1}(s) \bigg)
\end{equation}
$V^*_{*,1} - V^k_{k,1}$ relates the optimal rewards of the MDP $M^*$ to those near optimal for $\tilde{M}_k$.
We can bound this difference by the planning accuracy $\sqrt{1/k}$ for PSRL in expectation, since $M^*$ and $M_k$ are equal in law, and for UCRL-Factored in high probability by optimism.
% This follows for PSRL by Lemma \ref{lem: fundamental}, and for UCRL-Factored whenever the true MDP lies within the confidence set $\Mc_k$ by the principle of OFU and the approximate MDP planner $\Gamma$.

We decompose the first term through repeated application of dynamic programming:
\begin{equation}
	\left(V^k_{k,1} - V^*_{k,1} \right) (s_{t_k+1}) = \sum_{i=1}^\tau \left( \Tc^k_{k,i} - \Tc^*_{k,i} \right) V^k_{k,i+1}(s_{t_k+i}) + \sum_{i=1}^\tau d_{t_k+1}.
\end{equation}
Where $d_{t_k+i} := \sum_{s \in \Sc} \left\{ P^*(s | x_{k,i}) (V^*_{k,i+1}-V^k_{k,i+1})(s) \right\} - (V^*_{k,i+1} - V^k_{k,i+1})(s_{t_k+i})$
% The second term captures the randomness in the transitions of the true MDP $M^*$ under policy $\mu_k$.
% The expected value of $(V^*_{k,i+1} - V^k_{k,i+1})(s_{t_k+i})$ given $H_{t_k+i}$ is precisely $\sum_{s \in \Sc} \left\{ P^*(s | x_{k,i}) (V^*_{k,i+1}-V^k_{k,i+1})(s) \right\}$
% so that $\Exp[d_{t_k+i}] = 0 $ for all $i$.
% To obtain high probability bounds for UCRL-Factored we note that $d_{t_k+i}$
is a martingale difference bounded by $\Psi_k$, the span of $V^k_{k,i}$.
For UCRL-Factored we can use optimism to say that $\Psi_k \le CD$ \cite{jaksch2010near} and apply the Azuma-Hoeffding inequality to say that:
\begin{equation}
\label{eq: d_t}
	\Prob \left( \sum_{k=1}^m \sum_{i=1}^\tau d_{t_k+i} > CD\sqrt{2T\log(2/\delta)} \right) \le \delta
\end{equation}

The remaining term is the one step Bellman error of the imagined MDP $\tilde{M}_k$.
Crucially this term only depends on states and actions $x_{k,i}$ which are actually observed.
We can now use the H\"{o}lder inequality to bound
\begin{equation}
\label{eq: err sums}
	\sum_{i=1}^\tau \left( \Tc^k_{k,i} - \Tc^*_{k,i} \right) V^k_{k,i+1}(s_{t_k+i}) \le
		\sum_{i=1}^\tau |\overline{R}^k(x_{k,i}) - \overline{R}^*(x_{k,i}) | + \frac{1}{2} \Psi_k \|P^k(\cdot|x_{k,i}) - P^*(\cdot|x_{k,i}) \|_1
\end{equation}



%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Factorization decomposition}

We aim to exploit the graphical structure $\Gc$ to create more efficient confidence sets $\Mc_k$.
It is clear from \eqref{eq: err sums} that we may upper bound the deviations of $\overline{R}^*,\overline{R}^k$ factor-by-factor using the triangle inequality.
Our next result, Lemma \ref{lem: factor bound}, shows we can also do this for the transition functions $P^*$ and $P^k$.
This is the key result that allows us to build confidence sets around each factor $P^*_j$ rather than $P^*$ as a whole.

\begin{lemma}[Bounding factored deviations]
\label{lem: factor bound} \hspace{0.000000001mm} \newline
Let the transition function class $\Pc \subseteq \Pc_{\Xc,\Sc}$ be factored over $\Xc = \Xc_1 \times .. \times \Xc_n$ and $\Sc = \Sc_1 \times .. \times \Sc_m$ with scopes  $Z_1, .. Z_m$.
Then, for any $P,\tilde{P} \in \Pc$ we may bound their L1 distance by the sum of the differences of their factorizations:
$$ \| P(x) - \tilde{P}(x) \|_1 \le \sum_{i=1}^m \|P_i(x[Z_i]) - \tilde{P}_i(x[Z_i]) \|_1 $$

\begin{proof}
We begin with the simple claim that for any $ \alpha_1, \alpha_2, \beta_1, \beta_2 \in (0,1]$:
\begin{eqnarray*}
	| \alpha_1 \alpha_2 - \beta_1 \beta_2 | &=& \alpha_2 \left| \alpha_1 - \frac{\beta_1 \beta_2}{\alpha_2} \right| \\
	&\le& \alpha_2 \left( \left|\alpha_1 - \beta_1 \right| + \left|\beta_1 - \frac{\beta_1 \beta_2}{\alpha_2} \right| \right) \\
	%&\le& \alpha_2 \left| \alpha_1 - \beta_1 \right| + \alpha_2 \frac{\beta_1}{\alpha_2} \left| \frac{\beta_1 \alpha_2}{\beta_1} - \beta_2 \right| \\
	&\le& \alpha_2 \left| \alpha_1 - \beta_1 \right| + \beta_1 \left| \alpha_2 - \beta_2 \right|
\end{eqnarray*}
This result also holds for any $ \alpha_1, \alpha_2, \beta_1, \beta_2 \in [0,1]$, where $0$ can be verified case by case.

We now consider the probability distributions $p, \tilde{p} $ over $\{1,..,d_1\}$ and $q,\tilde{q} $ over $\{1,..,d_2\}$.
We let $Q = p q^T, \tilde{Q} = \tilde{p} \tilde{q}^T$ be the joint probability distribution over $\{1,..,d_1\} \times \{1,..,d_2\}$.
Using the claim above we bound the L1 deviation $\| Q - \tilde{Q} \|_1$ by the deviations of their factors:
\begin{eqnarray*}
	\| Q - \tilde{Q} \|_1 %&=& \sum_{i=1}^{d_1} \sum_{j=1}^{d_2} | Q_{i,j} - \tilde{Q}_{i,j} | \\
	&=& \sum_{i=1}^{d_1} \sum_{j=1}^{d_2} | p_i q_j - \tilde{p}_i \tilde{q}_j | \\
	&\le& \sum_{i=1}^{d_1} \sum_{j=1}^{d_2} q_j | p_i - \tilde{p}_i | + \tilde{p}_i | q_j - \tilde{q}_j | \\
	%&=& \sum_{i=1}^{d_1} |p_i - \tilde{p}_i | + \sum_{j=1}^{d_2} |q_j - \tilde{q}_j | \\
	&=& \| p - \tilde{p} \|_1 + \|q - \tilde{q} \|_1
\end{eqnarray*}
% Applying this result $m$ times to the factored transition functions $P$ and $\tilde{P}$ we recover our desired result.
We conclude the proof by applying this $m$ times to the factored transitions $P$ and $\tilde{P}$.
\end{proof}
\end{lemma}



%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Concentration guarantees for $\Mc_k$}
We now want to show that the true MDP lies within $\Mc_k$ with high probability.
Note that posterior sampling will also allow us to then say that the sampled $M_k$ is within $\Mc_k$ with high probability too.
In order to show this, we first present a concentration result for the L1 deviation of empirical probabilities.

\begin{lemma}[L1 bounds for the empirical transition function]
\label{lem: weissman} \hspace{0.000000001mm} \newline
For all finite sets $\Xc$, finite sets $\Yc$, function classes $\Pc \subseteq \Pc_{\Xc,\Yc}$ then for any $x \in \Xc$, $\epsilon > 0$ the deviation the true distribution $P^*$ to the empirical estimate after $t$ samples $\hat{P}_t$ is bounded:
\begin{equation*}
	\Prob \left( \| P^*(x) - \hat{P}_t(x) \|_1 \ge \epsilon \right) \le \exp \left( |\Yc| \log(2) - \frac{n_t(x) \epsilon^2}{2} \right)
\end{equation*}
\begin{proof}
This is a relaxation of the result proved by Weissman \cite{weissman2003inequalities}.
\end{proof}
\end{lemma}
Lemma \ref{lem: weissman} ensures that for any $x \in \Xc$
$\Prob ( \| P_j^*(x) - \hat{P_j}_t(x) \|_{1} \ge \sqrt{\frac{2 | \Sc_j |}{n_t(x)}\log\left( \frac{2}{\delta'} \right)} \ ) \le \delta'$.
We then define $d^{P_j}_{t_k} = 2 | \Sc_i | \log( 2/ \delta'_{k,j} )$ with $\delta'_{k,j} = \delta / (2 m | \Xc[Z^P_j] | k^2)$.
Now using a union bound we conclude % together with the fact that $\sum_{n=1}^\infty 1/n^2 = \pi^2/6 < 2$
$\Prob ( P^*_j \in \Pc^j_t(d_{t_k}^{P_j}) \ \forall k \in \Nat,j=1,..,m ) \ge 1 - \delta $.

% The proof for sub $\sigma$-gaussian random variables follows from the definition and resultant tail bounds.
% $\epsilon \in \Real$ is a sub $\sigma$-gaussian random variable
% $ \iff \forall t \in \Real, \ \  \Exp\left[ \exp(t\epsilon) \right] \le \exp\left(\frac{\sigma^2 t^2}{2}\right) $.

\begin{lemma}[Tail bounds for sub $\sigma$-gaussian random variables]
\label{lem: tail}
\hspace{0.000000001mm} \newline
If $\{ \epsilon_i \}$ are all independent and sub $\sigma$-gaussian then $\forall \beta \ge 0$:
$$\Prob \left( \frac{1}{n} | \sum_{i=1}^n \epsilon_i | > \beta \right) \le \exp\left( \log(2) -\frac{n \beta^2}{2 \sigma^2} \right) $$
\end{lemma}
% Lemma \ref{lem: tail} ensures that for any $x \in \Xc$, $i=1,..,l$
% $\Prob \left( | \overline{R}_i^*(x) - \hat{\overline{R}_i}_t(x) | \ge \sqrt{\frac{2 \sigma^2}{n_t(x)}\log\left( \frac{2}{\delta'} \right)} \ \right) \le \delta'$.
A similar argument now ensures that
$\Prob \left( \overline{R}^*_i \in \Rc^i_t(d_{t_k}^{R_i}) \ \forall k \in \Nat,i=1,..,l  \right) \ge 1 - \delta $, and so
\begin{equation}
\label{eq: conf}
	\Prob \bigg( M^*\in \Mc_k \ \forall k \in \Nat  \bigg) \ge 1 - 2\delta
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Regret bounds}
\label{sec: bounds}
We now have all the necessary intermediate results to complete our proof.
We begin with the analysis of PSRL.
Using equation \eqref{eq: conf} and the fact that $M^*,M_k$ are equal in law by posterior sampling, we can say that $\Prob( M^*, M_k \in \Mc_k \forall k \in \Nat) \ge 1-4\delta$.
The contributions from regret in planning function $\Gamma$ are bounded by $ \sum_{k=1}^m \sqrt{\tau/k} \le 2\sqrt{T}$.
From here we take equation \eqref{eq: err sums}, Lemma \ref{lem: factor bound} and Theorem \ref{thm: widths} to say that for any $\delta > 0 $:
\begin{eqnarray*}
	\Exp \left[\mathrm{Regret}(T, \pi^{\rm PS}_{\tau}, M^*) \right] &\le& 4\delta T +2 \sqrt{T} +
		\sum_{i=1}^l \left\{ 4(\tau C |\Xc[Z^R_i]| + 1) + 4\sqrt{2d_T^{R_i} | \Xc[Z^R_i]| T} \right\} \\
	&& \hspace{-25mm} + \sup_{k=1,..,L} \big( \Exp[\Psi_k | M_k, M^* \in \Mc_k] \big) \times \sum_{j=1}^m \left\{ 4(\tau |\Xc[Z^P_j]| + 1) + 4\sqrt{2d_T^{P_j} | \Xc[Z^P_j]| T} \right\}
\end{eqnarray*}
Let $A = \{ M^*, M_k \in \Mc_k \}$, since $\Psi_k \ge 0$ and by posterior sampling $\Exp [ \Psi_k ] = \Exp [\Psi]$ for all $k$:
$$\Exp[ \Psi_k \vert A  ] \le \Prob(A)^{-1} \Exp[\Psi] \le \left( 1 - \frac{4 \delta}{k^2} \right)^{-1} \Exp[ \Psi ] = \left(1 + \frac{4\delta}{k^2 - 4\delta} \right) \Exp[ \Psi ] \le \left(1 + \frac{4\delta}{1 - 4\delta} \right) \Exp[ \Psi ].$$
Plugging in $d_T^{R_i}$ and $d_T^{P_j}$ and setting $\delta=1/T$ completes the proof of Theorem \ref{thm: reg PSRL}.
The analysis of UCRL-Factored and Theorem \ref{thm: reg UCRL-Factored} follows similarly from \eqref{eq: d_t} and \eqref{eq: conf}.
Corollaries \ref{cor: reg PSRL} and \ref{cor: reg UCRL-Factored} follow from substituting the structure $\Qc$ and upper bounding the constant and logarithmic terms.
This is presented in detail in Appendix \ref{sec: Clean symmetric bounds}.

\section{Conclusion}
We present the first algorithms with near-optimal regret bounds in factored MDPs.
Many practical problems for reinforcement learning will have extremely large state and action spaces, this allows us to obtain meaningful performance guarantees even in previously intractably large systems.
However, our analysis leaves several important questions unaddressed.
First, we assume access to an approximate FMDP planner that may be computationally prohibitive in practice.
Second, we assume that the graph structure is known a priori but there are other algorithms that seek to learn this from experience \cite{strehl2007efficient,diuk2009adaptive}.
Finally, we might consider dimensionality reduction in large MDPs more generally, where either the rewards, transitions or optimal value function are known to belong in some function class $\Fc$ to obtain bounds that depend on the dimensionality of $\Fc$.

\subsubsection*{Acknowledgments}
Osband is supported by Stanford Graduate Fellowships courtesy of PACCAR inc.
This work was supported in part by Award CMMI-0968707 from the National Science Foundation.


%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\newpage
\small{
\bibliography{referenceInformation.bib}
\bibliographystyle{unsrt}
}
\newpage




%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\appendix
\section{Bounding the widths of confidence sets}
\label{sec: widths}
We present elementary arguments which culminate in a proof of Theorem \ref{thm: widths}.

\begin{lemma}[Concentration results for $\sqrt{d_T / n_t(x)}$]
\label{lem: rad} \hspace{0.000000001mm} \newline
For all finite sets $\Xc$ and any $d_T, \epsilon \ge 0$:
$$\sum_{t=1}^T \Ind \left\{ \sqrt{ d_T / n_t(x_t) } > h(d_T,\epsilon) \right\} \le
	\sum_{t=1}^T \Ind \left\{ \sqrt{ d_T/n_t(x_t) } > \epsilon \right\} + |\Xc|,$$
Where $h(d_T, \epsilon) := \sqrt{d_T \epsilon^2 / (d_T + \epsilon^2)}$.
\end{lemma}
\begin{proof}
Let $(x_{s_1},..,x_{s_K})$ be the largest subsequence of $x^T_1$ such that $\sqrt{d_T / n_{s_i}(x_{s_i})}\in ( h(d_T,\epsilon), \epsilon] \ \forall i$.
Now for any $x \in \Xc$, let $\mathcal{T}_x = \{ s_i \ | \ x_{s_i} = x \}$.
Suppose there exist two distinct elements $\sigma, \rho \in \mathcal{T}_x$ with $\sigma < \rho$ so that $n_\rho(x) \ge n_\sigma(x) + 1$.
We note that for any $n \in \Real_+, \  h(d_T, \sqrt{d_T / n}) = \sqrt{d_T / (n+1)}$ so that:
$$ \epsilon \ge \sqrt{ d_T / n_\sigma(x) } \implies h(d_T, \epsilon) \ge \sqrt{ d_T / (n_\sigma(x) + 1) } \ge \sqrt{ d_T / n_\rho(x) } $$
This contradicts our assumption $\sqrt{d_T / n_\rho(x)} \in ( h(d,\epsilon), \epsilon]$ and so we must conclude that $| \mathcal{T}_x| \le 1$ for all $ x \in \Xc$.
This means that $(x_{s_1},..,x_{s_K})$ forms a subsequence of unique elements in $\Xc$, the total length of which must be bounded by $| \Xc | $.
\end{proof}

We now provide a corollary of this result which allows for episodic delays in updating visit counts $n_t(x)$.
We imagine that we will only update our counts every $\tau$ steps.

\begin{corollary}[Concentration results for $\sqrt{d_T / n_{t_k}(x)}$ in the episodic setting]
\label{cor: rad ep} \hspace{0.000000001mm} \newline
Let us associate times within episodes of length $\tau$, $t = t_k+i$ for $i=1,..,\tau$ and $T = M \times \tau$.
For all finite sets $\Xc$ and any $d_T, \epsilon \ge 0$:
$$\sum_{k=1}^M \sum_{i=1}^\tau \Ind \left\{ \sqrt{ d_T / n_{t_k}(x_{t_k+i}) } > h^{(\tau)}(d_T,\epsilon) \right\} \le
	\sum_{k=1}^M \sum_{i=1}^\tau  \Ind \left\{ \sqrt{ d_T / n_{t_k}(x_{t_k+i}) } > \epsilon \right\} + 2\tau |\Xc|,$$
Where $h^{(\tau)}(d_T,\epsilon)$ is the $\tau$-fold composition of $h(d_T,\cdot)$ acting on $\epsilon$.
\end{corollary}
\begin{proof}
By an argument of visiting times similar to lemma \ref{lem: rad} we can see that the worst case scenario for the episodic case $\sum_{k=1}^M \sum_{i=1}^\tau \Ind \left\{ \sqrt{ d_T / n_{t_k}(x_{t_k+i}) } > h^{(\tau)}(d_T,\epsilon) \right\}$ is to visit each $x$ exactly $\tau-1$ times before the start of an episode, and then spend the entirety of the following episode within the state.
Here we have upper bounded $2\tau-1$ by $2\tau$ and $|\Xc|-1$ by $|\Xc|$ to complete our result.
\end{proof}

It will be useful to define notion of radius for each confidence set at each $x \in \Xc$,
$r_{\Fc_t}(x) := \sup_{f \in \Fc_t} \| (f - \hat{f}_t)(x) \|.$
By the triangle inequality, we have $w_{\Fc_t}(x) \le 2 r_{\Fc_t}(x)$ for all $x \in \Xc$.

%-------
\begin{lemma}[Bounding the number of large radii]
\label{lem: large rad} \hspace{0.000000001mm} \newline
Let us write $\Fc_k$ for $\Fc_{t_k}$ and associate times within episodes of length $\tau$, $t = t_k+i$ for $i=1,..,\tau$ and $T = M \times \tau$.
For all finite sets $\Xc$, measurable spaces $(\Yc,\Sigma_{\Yc})$, function classes $\Fc \subseteq \Mc_{\Xc,\Yc}$, non-decreasing sequences
$\{d_t : t \in \Nat \}$, any $T \in \Nat$ and $\epsilon >0$:
$$ \sum_{k=1}^M \sum_{i=1}^\tau \Ind\{r_{\Fc_k}(x_{t_k+i}) > \epsilon \} < \left( \frac{d_T}{\tau \epsilon^2} +1 \right) 2\tau | \Xc | $$

\begin{proof}
By construction of $\Fc_t$ and noting that $d_t$ is non-decreasing in $t$, we can say that $r_{\Fc_k}(x_t) \le \sqrt{d_T / n_{t_k}(x_t)} $ for all $ t = 1,..,T$ so that
$$ \sum_{k=1}^M \sum_{i=1}^\tau \Ind\{r_{\Fc_k}(x_{t+k+1}) > \epsilon \}
	 \le \sum_{k=1}^M \sum_{i=1}^\tau  \Ind \left\{ \sqrt{ d_T / n_{t_k}(x_{t_k+i}) } > \epsilon \right\}  .$$

Now let $g(\epsilon) = \sqrt{d_T \epsilon^2 / (d_T - \tau \epsilon^2)}$ be the $\epsilon$-inverse of $h^{(\tau)}(d_T,\epsilon)$ such that $g( h^{(\tau)}(d_T,\epsilon)) = \epsilon$.
Applying Corollary \ref{cor: rad ep} to our expression $n$ times repeatedly we can say:
$$\sum_{k=1}^M \sum_{i=1}^\tau  \Ind \left\{ \sqrt{ d_T / n_{t_k}(x_{t_k+i}) } > \epsilon \right\} \le
	\sum_{k=1}^M \sum_{i=1}^\tau  \Ind \left\{ \sqrt{ d_T / n_{t_k}(x_{t_k+i}) } > g^{(n)}(\epsilon) \right\} + 2n \tau |\Xc|.$$
Where $g^{(n)}(\epsilon)$ denotes the composition of $g(\cdot)$ $n$-times acting on $\epsilon$.
If we take $n$ to be the lowest integer such that $g^{(n)}(\epsilon) > \sqrt{d_T / \tau}$ then,
$\sum_{k=1}^M \sum_{i=1}^\tau  \Ind \left\{ \sqrt{ d_T / n_{t_k}(x_{t_k+i}) } > g^{(n)}(\epsilon) \right\} \le 2\tau |\Xc| $ so that the whole expression is bounded by $ \left( n + 1 \right) 2\tau | \Xc |$.
Note that for all $N \in \Real_+$, $g(\sqrt{d_T / N}) = \sqrt{d_T / (N-\tau)}$, if we write $\epsilon = \sqrt{d_T / N_1}$ then $n \le N_1 / \tau = \frac{d_T}{ \tau \epsilon^2}$, which completes the proof.

\end{proof}
\end{lemma}

Using these results we are finally able to complete our proof of Theorem \ref{thm: widths}
We first note that, via the triangle inequality $ \sum_{k=1}^M \sum_{i=1}^\tau w_{\mathcal{F}_{k}}(x_{t_k+i}) \le 2 \sum_{k=1}^M \sum_{i=1}^\tau r_{\mathcal{F}_{k}}(x_{t_k+i})$.
We streamline our notation by letting $r_{k,i} = r_{\mathcal{F}_{k}}(x_{t_k+i}) $.
Reordering the sequence $(r_{1,1}, .. , r_{M,\tau}) \rightarrow (r_{i_1}, .. ,r_{i_T})$ such $r_{i_1} \ge .. \ge r_{i_T}$ we have that:
$$ \sum_{k=1}^M \sum_{i=1}^\tau r_{\mathcal{F}_{k}}(x_{t_k+i})  = \sum_{t=1}^T r_{i_t} \le 1 + \sum_{i=1}^T r_{i_t} \Ind \{ r_{i_t} \ge T^{-1} \}. $$
We can see that $r_{i_t} > \epsilon \ge T^{-1} \ \iff \sum_{i=1}^T \Ind \{ r_{i_t} \ge \epsilon\} \ge t$.
From Lemma \ref{lem: large rad} this means that $t \le \left( \frac{d_T}{\tau \epsilon^2} + 1 \right) 2\tau |\Xc|$, so that
$\epsilon \le \sqrt{ \frac{2|\Xc| d_T}{t- 2\tau |\Xc|} }$.
This means that $r_{i_t} \le \min \{ C_\Fc , \sqrt{ \frac{2|\Xc| d_T}{t- 2\tau |\Xc|} } \}$.
Therefore,
\begin{eqnarray*}
	\sum_{i=1}^T r_{i_t} \Ind \{ r_{i_t} \ge T^{-1} \}
	&\le& 2\tau C_\Fc |\Xc| + \sum_{t=2\tau |\Xc| +1}^T \sqrt{\frac{2d_T |\Xc|}{t - \tau |\Xc|}} \\
	&\le& 2\tau C_\Fc |\Xc| +  \int_0^T \sqrt{ \frac{2d_T |\Xc|}{t} } \,dt \\
	&\le& 2\tau C_\Fc |\Xc| + 2 \sqrt{2d_T |\Xc| T}
\end{eqnarray*}
Which completes the proof of Theorem \ref{thm: widths}.





%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Clean bounds for the symmetric problem}
\label{sec: Clean symmetric bounds}
We now provide concrete clean upper bounds for Theorems \ref{thm: reg PSRL} and \ref{thm: reg UCRL-Factored} in the simple symmetric case
$l+1=m$, $C=\sigma=1$, $| \Sc_i | = |\Xc_i | = K$ and $|Z^R_i| = |Z^P_i | = \zeta$ for all suitable $i$ and write $J = K^\zeta$.
For a non-trivial problem setting we assume that $K \ge 2$, $m\ge2$, $\tau \ge 2$.

From Section \ref{sec: bounds} we have that
\begin{eqnarray*}
	\Exp \left[\mathrm{Regret}(T, \pi^{\rm PS}_{\tau}, M^*) \right] &\le& 4 +2 \sqrt{T} +
		m \left\{ 4(\tau J + 1) + 4\sqrt{8 \log(4mJ T^2/\tau) J T} \right\} \\
		&&+ \  \Exp[\Psi]\left(1 + \frac{4}{T-4}\right) m \left\{ 4(\tau J + 1) + 4\sqrt{8 K \log(4mJ T^2 / \tau) J T} \right\}
\end{eqnarray*}
Through looking at the constant term we know that the bounds are trivially satisfied for all $T\le 56$, from here we can certainly upper bound $4/(T-4) \le 1/13$.
From here we can say that:
\begin{eqnarray*}
	\Exp \left[\mathrm{Regret}(T, \pi^{\rm PS}_{\tau}, M^*) \right] &\le& \left\{ 4 + 4m\left(1 + \frac{14}{13} \Exp[\Psi]\right)(\tau J +1) \right\} \\
		&& + \sqrt{T} \left\{ 2 + 4\sqrt{8J \log(4mJ T^2/\tau)} + 4\sqrt{8 JK \log(4mJ T^2/\tau)} \frac{14}{13} \Exp[\Psi] \right\} \\
	&\le& 5 \left( 1 + \Exp[\Psi] \right) m \tau J + \sqrt{T} \left\{ 12\sqrt{J\log(2mJ T)} + 12\Exp[\Psi] \sqrt{JK\log(2mJ T)} \right\} \\
	&\le& 5 \left( 1 + \Exp[\Psi] \right) m \tau J + 12m\left( 1 + \Exp[ \Psi] \sqrt{K} \right) \sqrt{J T\log(2mJ T)} \\
	&\le& \min(5 m \tau^2 J, T) + 12m \tau \sqrt{J K T \log(2mJ T)} \\
	&\le& 15m \tau \sqrt{J K T \log(2mJ T)}
\end{eqnarray*}
Where in the last steps we have used that $\Psi \le \tau$ and $\min(a,b) \le \sqrt{ab}$.
We now repeat a similar procedure of upper bounds for UCRL-Factored, immediately replicating $D$ by $\tau$ in our analysis to say that with probability $\ge 1- 3\delta$:
\begin{eqnarray*}
	\mathrm{Regret}(T, \pi^{\rm UC}_{\tau}, M^*) &\le& \tau \sqrt{2T \log(2/\delta)} + 2\sqrt{T} +
		m \left\{ 4(\tau J + 1) + 4\sqrt{8 \log(4mJ T/\delta) J T} \right\} \\
		&&+ \  \tau m \left\{ 4(\tau J + 1) + 4\sqrt{8 K \log(4mJ T / \delta) J T} \right\}\\
		&\le& (1+\tau)m4(\tau J + 1) + \\
		  &&\sqrt{T} \left\{ \tau \sqrt{2 \log(2/\delta)} + 2 +  m4\sqrt{8 \log(4mJ T / \delta) J} + \tau m 4 \sqrt{8 \log(4mJ T / \delta) JK} \right\} \\
		&\le& 5(1+\tau)m\tau J + 12m(1 + \tau \sqrt{K}) \sqrt{J T \log(4mJT / \delta)} \\
		&\le& 15m \tau \sqrt{J K T \log(4mJ T / \delta)}
\end{eqnarray*}
Where in the last step we used a similar argument

%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


\end{document}




